{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by Backprop: Mushroom Contextual Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Background - Multi-armed and Contextual Bandit problems:__ <br>\n",
    "_“Bandit”_ in _“multi-armed bandits”_ comes from _“one-armed bandit”_ machines used in a casino. Imagine that you are in a casino with many one-armed bandit machines. Each machine has a different probability of a win. Your goal is to maximize total payout. You can pull a limited number of arms, and you don’t know which bandit to use to get the best payout. The problem involves exploration/exploitation tradeoff: you should balance between trying different bandits to learn more about an expected payout of every machine, but you also want to exploit the best bandit you know about more. While multi-armed bandits select the strategy which maximises the expected reward without taking into consideration the state of the environment, _contextual bandits_ output an action based on the context.\n",
    "\n",
    "__Mushroom Case Stuty:__ <br>\n",
    "We are provided with a list of 8124 mushrooms, each having 22 features (characteristcs of the mushroom) and 1 label (poisonous or edible). Our agent can carry out 2 actions: eat a mushroom or not eat a mushroom. The problem context is the vector of features which is associated with the mushroom which the agent is about to eat/non eat.\n",
    "If our agent eats an edible mushroom, it receives a reward of 5. If the agent eats a poisonous mushroom, it receives a reward of -35 with 0.5 probability and a reward of 5 with 0.5 probaility. If the agent doesn't eat, it receives a reward o 0.\n",
    "\n",
    "We are also provided an _oracle_. The oracle always selects the right action, receiving a reward of 5 when it eats an edible mushroom, and a reward of 0 when it doesn't eat.\n",
    "\n",
    "__Objective:__ <br>\n",
    "Create a BNN which minimises the _cumulative regret_ of the agent. Regreat measures the difference between the oracle and our agent's reward.\n",
    "\n",
    "__BNN Architecture:__\n",
    "- 2 hidden layers\n",
    "- Each layer has 100 rectified units\n",
    "- Vector input: vector consisting of the mushroom features (context) and a one of _K_ encoding of the action\n",
    "- Output: single scalar representing the expected reward of the given action in the given context\n",
    "\n",
    "__Additional Important Information:__\n",
    "- To calculate the expected reward for an action, Google samples twice the weights and averages the outputs\n",
    "- To train the network, Google keeps a buffer of 4096 mushrooms.\n",
    "- For training, Google randomly draws minibatches of size 64 for 64 training steps.\n",
    "- After every training stage, the agent interacts with a new mushroom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from BayesBackpropagation import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Preparation:__ ## \n",
    "Input is the data file. Output is an array of one-hot encoded labels (y, shape: 8124) and an array of context features (x, shape: 8124 elements, each with 117 features). 117 features derive from doing a one-hot encoding on the feature classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from file\n",
    "df = pd.read_csv(os.getcwd() + '/agaricus-lepiota.data', sep=',', header=None,\n",
    "             error_bad_lines=False, warn_bad_lines=True, low_memory=False)\n",
    "\n",
    "# Set pandas to output all of the columns in output\n",
    "df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n",
    "         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n",
    "\n",
    "# Split context from label\n",
    "X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n",
    "# Put the class values (0th column) into Y\n",
    "Y = df['class']\n",
    "\n",
    "# Transform labels into one-hot encoded array\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "y = le.transform(Y)\n",
    "\n",
    "# Temporary variable to avoid error \n",
    "x_tmp = pd.DataFrame(X,columns=[X.columns[0]])\n",
    "\n",
    "# Encode each feature column and add it to x_train \n",
    "for colname in X.columns:\n",
    "    le.fit(X[colname])\n",
    "    #print(colname, le.classes_)\n",
    "    x_tmp[colname] = le.transform(X[colname])\n",
    "\n",
    "# Produce mushroom array: 8124 mushrooms, each with 117 one-hot encoded features\n",
    "oh = preprocessing.OneHotEncoder(categorical_features='all')\n",
    "oh.fit(x_tmp)\n",
    "x = oh.transform(x_tmp).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Reward function:__ ##\n",
    "Implemented as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 117)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWARD FUNCTION\n",
    "\n",
    "def get_reward(eaten, edible):\n",
    "    # REWARDS FOR AGENT\n",
    "    #  Eat poisonous mushroom\n",
    "    if not eaten:\n",
    "        return 0\n",
    "    if eaten and edible:\n",
    "        return 5\n",
    "    elif eaten and not edible:\n",
    "        return (5 if np.random.rand() > 0.5 else -35)\n",
    "\n",
    "def oracle_reward(edible):\n",
    "    return 5*edible    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mushroom Buffer\n",
    "Generate buffer to keep 4096 elements used to train the Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the buffer, each element includes an action, context and reward\n",
    "\n",
    "def init_data():\n",
    "    contexts, types, optimal_rewards = [], [], []\n",
    "    for i in np.random.choice(range(len(x)), 10000):\n",
    "        contexts.append(x[i])\n",
    "        types.append(y[i])\n",
    "        optimal_rewards.append(oracle_reward(y[i]))\n",
    "    return contexts, types, optimal_rewards\n",
    "\n",
    "contexts, types, optimal_rewards = init_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available?:  False\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "print(\"Cuda available?: \",torch.cuda.is_available())\n",
    "\n",
    "PI = 0.5\n",
    "SIGMA_1 = torch.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.FloatTensor([math.exp(-6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "Var = lambda x, dtype=torch.FloatTensor: Variable(torch.from_numpy(x).type(dtype))\n",
    "\n",
    "\n",
    "class MushroomNet():\n",
    "    def __init__(self, label = 'MushNet', n_weight_sampling=2):\n",
    "        self.label = label\n",
    "        self.n_weight_sampling = n_weight_sampling\n",
    "        self.epsilon = 0\n",
    "        self.net = None\n",
    "        self.loss, self.optimizer = None, None\n",
    "        self.cum_regrets = [0]\n",
    "        self.bufferX = []\n",
    "        self.bufferY = []\n",
    "        self.trainingX = []\n",
    "        self.trainingY = []\n",
    "        \n",
    "    # Use NN to decide next action    \n",
    "    def try_ (self, mushroom):\n",
    "        samples = self.n_weight_sampling\n",
    "        context, edible = contexts[mushroom], types[mushroom]\n",
    "        try_eat = Var(np.concatenate((context, [1, 0])))\n",
    "        try_reject = Var(np.concatenate((context, [0, 1])))\n",
    "        \n",
    "        # Calculate rewards using model\n",
    "        with torch.no_grad():\n",
    "            r_eat = np.mean([self.net.forward(try_eat).numpy() for _ in range(samples)])\n",
    "            r_reject = np.mean([self.net.forward(try_reject).numpy() for _ in range(samples)])\n",
    "                         \n",
    "        # Take random action for epsilon greedy agents, calculate agent's reward\n",
    "        eaten = r_eat > r_reject\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            eaten = (np.random.rand()<.5)\n",
    "        agent_reward = get_reward(eaten, edible)\n",
    "        \n",
    "        # Get rewards and update buffer\n",
    "        if eaten:\n",
    "            action = [1, 0]\n",
    "        else:\n",
    "            action = [0, 1]\n",
    "        self.bufferX.append(np.concatenate((context, action)))\n",
    "        self.bufferY.append(agent_reward)\n",
    "    \n",
    "        # Calculate regret\n",
    "        oracle = oracle_reward(edible)\n",
    "        regret = oracle - agent_reward\n",
    "        self.cum_regrets.append(self.cum_regrets[-1]+regret) \n",
    "    \n",
    "        \n",
    "    # Function for generating a minibatch\n",
    "    def generate_minibatch(self, trainingX, trainingY):\n",
    "        bX = []\n",
    "        bY = []\n",
    "        for i in range(64):\n",
    "            random = np.random.randint(0, len(trainingX))\n",
    "            bX.append(self.trainingX[random])\n",
    "            bY.append(self.trainingY[random])\n",
    "            #print(\"random mushroom is:\" + str(random))\n",
    "            #print(\"len of bX is:\" + str(len(bX)))\n",
    "        return bX, bY\n",
    "    \n",
    "    # Feed next mushroom\n",
    "    def update(self, mushroom):\n",
    "        self.try_(mushroom)\n",
    "        #print(\"len of buffer is: \" + str(len(self.bufferX)))\n",
    "        self.trainingX = self.bufferX[-4096:]\n",
    "        #print(\"len of trainingX is: \" + str(len(self.trainingX)))\n",
    "        self.trainingY = self.bufferY[-4096:]\n",
    "        for minibatch in range(64):\n",
    "            bX, bY = self.generate_minibatch(self.trainingX, self.trainingY)\n",
    "            self.net.zero_grad()\n",
    "            self.loss(Var(np.asarray(bX)), Var(np.asarray(bY))).backward()\n",
    "            self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for BBB agent\n",
    "class BBB_MNet(MushroomNet):\n",
    "    def __init__(self, label):\n",
    "        super().__init__(label)\n",
    "        self.net = BayesianNetwork(inputSize = x.shape[1]+2,\n",
    "                          CLASSES = 1, \n",
    "                          layers=np.array([100,100]), \n",
    "                          activations = np.array(['relu','relu','none']), \n",
    "                          SAMPLES = 2, \n",
    "                          BATCH_SIZE = 64,\n",
    "                          NUM_BATCHES = 64,\n",
    "                          hasScalarMixturePrior=True,\n",
    "                          PI=PI,\n",
    "                          SIGMA_1 = SIGMA_1,\n",
    "                          SIGMA_2 = SIGMA_2\n",
    "                          ).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr = 0.5)\n",
    "        self.loss = lambda data, target:self.net.BBB_loss(data, target)\n",
    "        \n",
    "        \n",
    "# Class for Greedy agents\n",
    "class EpsGreedyMlp(MushroomNet):\n",
    "    def __init__(self, epsilon=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_weight_sampling = 1\n",
    "        self.epsilon = epsilon\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Linear(x.shape[1]+2, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr = 0.001)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.loss = lambda data, target: self.mse(self.net.forward(data), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_nets = {'bbb':BBB_MNet(label = 'BBB'),\n",
    "                 'e0':EpsGreedyMlp(epsilon=0, label = 'Greedy'),\n",
    "                 'e1':EpsGreedyMlp(epsilon=0.01, label = '1% Greedy'),\n",
    "                 'e5':EpsGreedyMlp(epsilon=0.05, label = '5% Greedy')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266d236782184901958f8bf9b4a95f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Fig</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Fig({\n",
       "    'data': [{'name': 'BBB', 'type': 'scatter', 'uid': '61245c8e-c301-4e03-8ed9-bf9a6b945706', 'x': [], 'y': []},\n",
       "             {'name': 'Greedy', 'type': 'scatter', 'uid': 'ea767dae-7e79-402b-b47f-268bf219f548', 'x': [], 'y': []},\n",
       "             {'name': '1% Greedy', 'type': 'scatter', 'uid': 'c23425f0-7bb3-4f54-88e8-7537cac19e58', 'x': [], 'y': []},\n",
       "             {'name': '5% Greedy', 'type': 'scatter', 'uid': '299e3eb9-bc74-4b9c-8dc9-8e8acedab8d2', 'x': [], 'y': []}],\n",
       "    'layout': {'title': {'text': 'Cumulative Regrets'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "class Fig(go.FigureWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__([go.Scatter(name = net.label, x=[], y=[]) \n",
    "                          for i, net in mushroom_nets.items()])\n",
    "        self.layout.title = 'Cumulative Regrets'\n",
    "    \n",
    "    def update(self, dic):\n",
    "        for j, (lb, y) in enumerate(dic.items()):\n",
    "            self.data[j].x = list(range(len(y)))\n",
    "            self.data[j].y = y\n",
    "    \n",
    "    def save(self, file_name = 'mushroom_regrets.csv'):\n",
    "        dic = {fd.name: fd.y for fd in self.data}\n",
    "        pd.DataFrame.from_dict(dic).to_csv(file_name)\n",
    "        \n",
    "        \n",
    "fig = Fig()\n",
    "fig  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "BBB",
         "type": "scatter",
         "uid": "61245c8e-c301-4e03-8ed9-bf9a6b945706",
         "x": [],
         "y": []
        },
        {
         "name": "Greedy",
         "type": "scatter",
         "uid": "ea767dae-7e79-402b-b47f-268bf219f548",
         "x": [],
         "y": []
        },
        {
         "name": "1% Greedy",
         "type": "scatter",
         "uid": "c23425f0-7bb3-4f54-88e8-7537cac19e58",
         "x": [],
         "y": []
        },
        {
         "name": "5% Greedy",
         "type": "scatter",
         "uid": "299e3eb9-bc74-4b9c-8dc9-8e8acedab8d2",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "title": {
         "text": "Cumulative Regrets"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline as py\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBB', 'Greedy', '1% Greedy', '5% Greedy']\n",
      "Step 0 Regrets [5, 5, 5, 5]\n",
      "Step 10 Regrets [20, 80, 25, 20]\n",
      "Step 20 Regrets [45, 175, 50, 35]\n",
      "Step 30 Regrets [70, 195, 70, 50]\n",
      "Step 40 Regrets [85, 210, 80, 60]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-3f756845dfd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmushroom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom_nets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcum_regrets\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmushroom_nets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-0d412a87ee2f>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, mushroom)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NB_STEPS = 10000\n",
    "\n",
    "\n",
    "offset = len(fig.data[0].x)\n",
    "print([net.label for lb, net in mushroom_nets.items()])\n",
    "\n",
    "for _ in range(offset, offset+NB_STEPS):\n",
    "    mushroom = _\n",
    "    for j, (key, net) in enumerate(mushroom_nets.items()):\n",
    "        net.update(mushroom)\n",
    "    fig.update({net.label: net.cum_regrets for lb, net in mushroom_nets.items()})\n",
    "    if _ % 10 == 0 :\n",
    "        print('Step',_,'Regrets',[net.cum_regrets[-1] for lb, net in mushroom_nets.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
