{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by Backprop: Mushroom Contextual Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Background - Multi-armed and Contextual Bandit problems:__ <br>\n",
    "_“Bandit”_ in _“multi-armed bandits”_ comes from _“one-armed bandit”_ machines used in a casino. Imagine that you are in a casino with many one-armed bandit machines. Each machine has a different probability of a win. Your goal is to maximize total payout. You can pull a limited number of arms, and you don’t know which bandit to use to get the best payout. The problem involves exploration/exploitation tradeoff: you should balance between trying different bandits to learn more about an expected payout of every machine, but you also want to exploit the best bandit you know about more. While multi-armed bandits select the strategy which maximises the expected reward without taking into consideration the state of the environment, _contextual bandits_ output an action based on the context.\n",
    "\n",
    "__Mushroom Case Stuty:__ <br>\n",
    "We are provided with a list of 8124 mushrooms, each having 22 features (characteristcs of the mushroom) and 1 label (poisonous or edible). Our agent can carry out 2 actions: eat a mushroom or not eat a mushroom. The problem context is the vector of features which is associated with the mushroom which the agent is about to eat/non eat.\n",
    "If our agent eats an edible mushroom, it receives a reward of 5. If the agent eats a poisonous mushroom, it receives a reward of -35 with 0.5 probability and a reward of 5 with 0.5 probaility. If the agent doesn't eat, it receives a reward o 0.\n",
    "\n",
    "We are also provided an _oracle_. The oracle always selects the right action, receiving a reward of 5 when it eats an edible mushroom, and a reward of 0 when it doesn't eat.\n",
    "\n",
    "__Objective:__ <br>\n",
    "Create a BNN which minimises the _cumulative regret_ of the agent. Regreat measures the difference between the oracle and our agent's reward.\n",
    "\n",
    "__BNN Architecture:__\n",
    "- 2 hidden layers\n",
    "- Each layer has 100 rectified units\n",
    "- Vector input: vector consisting of the mushroom features (context) and a one of _K_ encoding of the action\n",
    "- Output: single scalar representing the expected reward of the given action in the given context\n",
    "\n",
    "__Additional Important Information:__\n",
    "- To calculate the expected reward for an action, Google samples twice the weights and averages the outputs\n",
    "- To train the network, Google keeps a buffer of 4096 mushrooms.\n",
    "- For training, Google randomly draws minibatches of size 64 for 64 training steps.\n",
    "- After every training stage, the agent interacts with a new mushroom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from BayesBackpropagation import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Preparation:__ ## \n",
    "Input is the data file. Output is an array of one-hot encoded labels (y, shape: 8124) and an array of context features (x, shape: 8124 elements, each with 117 features). 117 features derive from doing a one-hot encoding on the feature classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from file\n",
    "df = pd.read_csv(os.getcwd() + '/agaricus-lepiota.data', sep=',', header=None,\n",
    "             error_bad_lines=False, warn_bad_lines=True, low_memory=False)\n",
    "\n",
    "# Set pandas to output all of the columns in output\n",
    "df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n",
    "         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n",
    "\n",
    "# Split context from label\n",
    "X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n",
    "# Put the class values (0th column) into Y\n",
    "Y = df['class']\n",
    "\n",
    "# Transform labels into one-hot encoded array\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "y = le.transform(Y)\n",
    "\n",
    "# Temporary variable to avoid error \n",
    "x_tmp = pd.DataFrame(X,columns=[X.columns[0]])\n",
    "\n",
    "# Encode each feature column and add it to x_train \n",
    "for colname in X.columns:\n",
    "    le.fit(X[colname])\n",
    "    #print(colname, le.classes_)\n",
    "    x_tmp[colname] = le.transform(X[colname])\n",
    "\n",
    "# Produce mushroom array: 8124 mushrooms, each with 117 one-hot encoded features\n",
    "oh = preprocessing.OneHotEncoder(categorical_features='all')\n",
    "oh.fit(x_tmp)\n",
    "x = oh.transform(x_tmp).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Reward function:__ ##\n",
    "Implemented as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 117)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWARD FUNCTION\n",
    "\n",
    "def get_reward(eaten, edible):\n",
    "    # REWARDS FOR AGENT\n",
    "    #  Eat poisonous mushroom\n",
    "    if not eaten:\n",
    "        return 0\n",
    "    if eaten and edible:\n",
    "        return 5\n",
    "    elif eaten and not edible:\n",
    "        return (5 if np.random.rand() > 0.5 else -35)\n",
    "\n",
    "def oracle_reward(edible):\n",
    "    return 5*edible    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mushroom Buffer\n",
    "Generate buffer to keep 4096 elements used to train the Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the buffer, each element includes an action, context and reward\n",
    "\n",
    "def init_data():\n",
    "    contexts, types, optimal_rewards = [], [], []\n",
    "    for i in np.random.choice(range(len(x)), 50000):\n",
    "        contexts.append(x[i])\n",
    "        types.append(y[i])\n",
    "        optimal_rewards.append(oracle_reward(y[i]))\n",
    "    return contexts, types, optimal_rewards\n",
    "\n",
    "contexts, types, optimal_rewards = init_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available?:  False\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "print(\"Cuda available?: \",torch.cuda.is_available())\n",
    "\n",
    "PI = 0.5\n",
    "SIGMA_1 = torch.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.FloatTensor([math.exp(-6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "Var = lambda x, dtype=torch.FloatTensor: Variable(torch.from_numpy(x).type(dtype))\n",
    "\n",
    "\n",
    "class MushroomNet():\n",
    "    def __init__(self, label = 'MushNet', n_weight_sampling=2):\n",
    "        self.label = label\n",
    "        self.n_weight_sampling = n_weight_sampling\n",
    "        self.epsilon = 0\n",
    "        self.net = None\n",
    "        self.loss, self.optimizer = None, None\n",
    "        self.cum_regrets = [0]\n",
    "        self.bufferX = []\n",
    "        self.bufferY = []\n",
    "        self.trainingX = []\n",
    "        self.trainingY = []\n",
    "        \n",
    "    # Use NN to decide next action    \n",
    "    def try_ (self, mushroom):\n",
    "        samples = self.n_weight_sampling\n",
    "        context, edible = contexts[mushroom], types[mushroom]\n",
    "        try_eat = Var(np.concatenate((context, [1, 0])))\n",
    "        try_reject = Var(np.concatenate((context, [0, 1])))\n",
    "        \n",
    "        # Calculate rewards using model\n",
    "        with torch.no_grad():\n",
    "            r_eat = np.mean([self.net.forward(try_eat).numpy() for _ in range(samples)])\n",
    "            r_reject = np.mean([self.net.forward(try_reject).numpy() for _ in range(samples)])\n",
    "                         \n",
    "        # Take random action for epsilon greedy agents, calculate agent's reward\n",
    "        eaten = r_eat > r_reject\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            eaten = (np.random.rand()<.5)\n",
    "        agent_reward = get_reward(eaten, edible)\n",
    "        \n",
    "        # Get rewards and update buffer\n",
    "        if eaten:\n",
    "            action = [1, 0]\n",
    "        else:\n",
    "            action = [0, 1]\n",
    "        self.bufferX.append(np.concatenate((context, action)))\n",
    "        self.bufferY.append(agent_reward)\n",
    "    \n",
    "        # Calculate regret\n",
    "        oracle = oracle_reward(edible)\n",
    "        regret = oracle - agent_reward\n",
    "        self.cum_regrets.append(self.cum_regrets[-1]+regret) \n",
    "    \n",
    "    # Function for generating a minibatch\n",
    "    def generate_minibatch(self, trainingX, trainingY):\n",
    "        bX = []\n",
    "        bY = []\n",
    "        for i in range(64):\n",
    "            random = np.random.randint(0, len(trainingX))\n",
    "            bX.append(self.trainingX[random])\n",
    "            bY.append(self.trainingY[random])      \n",
    "        return bX, bY\n",
    "    \n",
    "    # Feed next mushroom\n",
    "    def update(self, mushroom):\n",
    "        self.try_(mushroom)\n",
    "        self.trainingX = self.bufferX[-4096:]\n",
    "        self.trainingY = self.bufferY[-4096:]\n",
    "        for minibatch in range(64):\n",
    "            bX, bY = self.generate_minibatch(self.trainingX, self.trainingY)\n",
    "            self.net.zero_grad()\n",
    "            self.loss(Var(np.asarray(bX)), Var(np.asarray(bY))).backward()\n",
    "            self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for BBB agent\n",
    "class BBB_MNet(MushroomNet):\n",
    "    def __init__(self, label):\n",
    "        super().__init__(label)\n",
    "        self.net = BayesianNetwork(inputSize = x.shape[1]+2,\n",
    "                          CLASSES = 1, \n",
    "                          layers=np.array([100,100]), \n",
    "                          activations = np.array(['relu','relu','none']), \n",
    "                          SAMPLES = 2, \n",
    "                          BATCH_SIZE = 64,\n",
    "                          NUM_BATCHES = 64,\n",
    "                          hasScalarMixturePrior=True,\n",
    "                          PI=PI,\n",
    "                          SIGMA_1 = SIGMA_1,\n",
    "                          SIGMA_2 = SIGMA_2\n",
    "                          ).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr = 0.5)\n",
    "        self.loss = lambda data, target:self.net.BBB_loss(data, target)\n",
    "        \n",
    "        \n",
    "# Class for Greedy agents\n",
    "class EpsGreedyMlp(MushroomNet):\n",
    "    def __init__(self, epsilon=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_weight_sampling = 1\n",
    "        self.epsilon = epsilon\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Linear(x.shape[1]+2, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "        self.bufferX, self.bufferY = init_buffer()\n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr = 0.001)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.loss = lambda data, target: self.mse(self.net.forward(data), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_nets = {'bbb':BBB_MNet(label = 'BBB'),\n",
    "                 'e0':EpsGreedyMlp(epsilon=0, label = 'Greedy'),\n",
    "                 'e1':EpsGreedyMlp(epsilon=0.01, label = '1% Greedy'),\n",
    "                 'e5':EpsGreedyMlp(epsilon=0.05, label = '5% Greedy')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af2cae0f02d47dc8d1f45a64bd86749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Fig</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Fig({\n",
       "    'data': [{'name': 'BBB', 'type': 'scatter', 'uid': '640311d1-0e6a-44f8-b19f-c6b9deaf2566', 'x': [], 'y': []},\n",
       "             {'name': 'Greedy', 'type': 'scatter', 'uid': '1985a829-3093-42c5-88cb-2ff896da17ba', 'x': [], 'y': []},\n",
       "             {'name': '1% Greedy', 'type': 'scatter', 'uid': '6d8c25a1-d062-4c59-b27c-eda20bd9d35d', 'x': [], 'y': []},\n",
       "             {'name': '5% Greedy', 'type': 'scatter', 'uid': '51d1cabb-93e1-4e87-802a-4f2aa23b467a', 'x': [], 'y': []}],\n",
       "    'layout': {'title': {'text': 'Cumulative Regrets'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "class Fig(go.FigureWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__([go.Scatter(name = net.label, x=[], y=[]) \n",
    "                          for i, net in mushroom_nets.items()])\n",
    "        self.layout.title = 'Cumulative Regrets'\n",
    "    \n",
    "    def update(self, dic):\n",
    "        for j, (lb, y) in enumerate(dic.items()):\n",
    "            self.data[j].x = list(range(len(y)))\n",
    "            self.data[j].y = y\n",
    "    \n",
    "    def save(self, file_name = 'mushroom_regrets.csv'):\n",
    "        dic = {fd.name: fd.y for fd in self.data}\n",
    "        pd.DataFrame.from_dict(dic).to_csv(file_name)\n",
    "        \n",
    "        \n",
    "fig = Fig()\n",
    "fig  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "BBB",
         "type": "scatter",
         "uid": "640311d1-0e6a-44f8-b19f-c6b9deaf2566",
         "x": [],
         "y": []
        },
        {
         "name": "Greedy",
         "type": "scatter",
         "uid": "1985a829-3093-42c5-88cb-2ff896da17ba",
         "x": [],
         "y": []
        },
        {
         "name": "1% Greedy",
         "type": "scatter",
         "uid": "6d8c25a1-d062-4c59-b27c-eda20bd9d35d",
         "x": [],
         "y": []
        },
        {
         "name": "5% Greedy",
         "type": "scatter",
         "uid": "51d1cabb-93e1-4e87-802a-4f2aa23b467a",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "title": {
         "text": "Cumulative Regrets"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline as py\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBB', 'Greedy', '1% Greedy', '5% Greedy']\n",
      "Step 0 Regrets [-5, 0, 0, 0]\n",
      "Step 10 Regrets [30, 15, 45, 35]\n",
      "Step 20 Regrets [45, 90, 50, 45]\n",
      "Step 30 Regrets [60, 105, 45, 60]\n",
      "Step 40 Regrets [95, 140, 50, 95]\n",
      "Step 50 Regrets [130, 175, 40, 130]\n",
      "Step 60 Regrets [150, 195, 105, 150]\n",
      "Step 70 Regrets [185, 230, 115, 185]\n",
      "Step 80 Regrets [205, 250, 120, 205]\n",
      "Step 90 Regrets [230, 275, 160, 230]\n",
      "Step 100 Regrets [260, 305, 170, 260]\n",
      "Step 110 Regrets [275, 320, 180, 270]\n",
      "Step 120 Regrets [305, 350, 230, 300]\n",
      "Step 130 Regrets [335, 380, 240, 330]\n",
      "Step 140 Regrets [360, 405, 240, 350]\n",
      "Step 150 Regrets [390, 435, 260, 380]\n",
      "Step 160 Regrets [410, 455, 265, 435]\n",
      "Step 170 Regrets [435, 480, 305, 460]\n",
      "Step 180 Regrets [465, 510, 325, 485]\n",
      "Step 190 Regrets [490, 535, 340, 510]\n",
      "Step 200 Regrets [510, 550, 350, 530]\n",
      "Step 210 Regrets [530, 570, 365, 550]\n",
      "Step 220 Regrets [545, 585, 380, 565]\n",
      "Step 230 Regrets [570, 605, 390, 585]\n",
      "Step 240 Regrets [605, 640, 415, 620]\n",
      "Step 250 Regrets [630, 665, 435, 645]\n",
      "Step 260 Regrets [665, 700, 465, 675]\n",
      "Step 270 Regrets [690, 725, 490, 700]\n",
      "Step 280 Regrets [715, 750, 510, 720]\n",
      "Step 290 Regrets [740, 775, 520, 740]\n",
      "Step 300 Regrets [760, 790, 530, 760]\n",
      "Step 310 Regrets [785, 815, 545, 780]\n",
      "Step 320 Regrets [825, 855, 565, 820]\n",
      "Step 330 Regrets [845, 870, 585, 840]\n",
      "Step 340 Regrets [865, 890, 600, 855]\n",
      "Step 350 Regrets [900, 925, 620, 885]\n",
      "Step 360 Regrets [920, 945, 625, 905]\n",
      "Step 370 Regrets [940, 960, 635, 955]\n",
      "Step 380 Regrets [970, 990, 655, 985]\n",
      "Step 390 Regrets [995, 1015, 670, 1010]\n",
      "Step 400 Regrets [1040, 1055, 700, 1055]\n",
      "Step 410 Regrets [1060, 1070, 710, 1075]\n",
      "Step 420 Regrets [1075, 1080, 705, 1090]\n",
      "Step 430 Regrets [1100, 1105, 725, 1115]\n",
      "Step 440 Regrets [1125, 1130, 735, 1140]\n",
      "Step 450 Regrets [1140, 1145, 750, 1155]\n",
      "Step 460 Regrets [1160, 1165, 755, 1175]\n",
      "Step 470 Regrets [1190, 1190, 775, 1195]\n",
      "Step 480 Regrets [1215, 1205, 780, 1220]\n",
      "Step 490 Regrets [1235, 1220, 795, 1235]\n",
      "Step 500 Regrets [1240, 1225, 795, 1240]\n",
      "Step 510 Regrets [1265, 1245, 805, 1265]\n",
      "Step 520 Regrets [1290, 1270, 815, 1290]\n",
      "Step 530 Regrets [1330, 1295, 820, 1330]\n",
      "Step 540 Regrets [1350, 1310, 825, 1350]\n",
      "Step 550 Regrets [1375, 1330, 830, 1375]\n",
      "Step 560 Regrets [1400, 1335, 835, 1400]\n",
      "Step 570 Regrets [1420, 1355, 840, 1420]\n",
      "Step 580 Regrets [1440, 1365, 845, 1440]\n",
      "Step 590 Regrets [1475, 1395, 870, 1475]\n",
      "Step 600 Regrets [1505, 1415, 870, 1505]\n",
      "Step 610 Regrets [1530, 1430, 870, 1530]\n",
      "Step 620 Regrets [1550, 1435, 880, 1550]\n",
      "Step 630 Regrets [1560, 1440, 885, 1560]\n",
      "Step 640 Regrets [1595, 1470, 895, 1630]\n",
      "Step 650 Regrets [1630, 1485, 895, 1660]\n",
      "Step 660 Regrets [1650, 1500, 895, 1680]\n",
      "Step 670 Regrets [1665, 1505, 900, 1695]\n",
      "Step 680 Regrets [1685, 1510, 905, 1715]\n",
      "Step 690 Regrets [1695, 1510, 910, 1725]\n",
      "Step 700 Regrets [1720, 1530, 925, 1750]\n",
      "Step 710 Regrets [1740, 1620, 935, 1770]\n",
      "Step 720 Regrets [1770, 1635, 945, 1800]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-357-99a2ecadf0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmushroom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom_nets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcum_regrets\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmushroom_nets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-352-dde7b894dc47>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, mushroom)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mbX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NB_STEPS = 1000\n",
    "\n",
    "\n",
    "offset = len(fig.data[0].x)\n",
    "print([net.label for lb, net in mushroom_nets.items()])\n",
    "\n",
    "for _ in range(offset, offset+NB_STEPS):\n",
    "    mushroom = _\n",
    "    for j, (key, net) in enumerate(mushroom_nets.items()):\n",
    "        net.update(mushroom)\n",
    "    fig.update({net.label: net.cum_regrets for lb, net in mushroom_nets.items()})\n",
    "    if _ % 10 == 0 :\n",
    "        print('Step',_,'Regrets',[net.cum_regrets[-1] for lb, net in mushroom_nets.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
