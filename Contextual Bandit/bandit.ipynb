{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by Backprop: Mushroom Contextual Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Background - Multi-armed and Contextual Bandit problems:__ <br>\n",
    "_“Bandit”_ in _“multi-armed bandits”_ comes from _“one-armed bandit”_ machines used in a casino. Imagine that you are in a casino with many one-armed bandit machines. Each machine has a different probability of a win. Your goal is to maximize total payout. You can pull a limited number of arms, and you don’t know which bandit to use to get the best payout. The problem involves exploration/exploitation tradeoff: you should balance between trying different bandits to learn more about an expected payout of every machine, but you also want to exploit the best bandit you know about more. While multi-armed bandits select the strategy which maximises the expected reward without taking into consideration the state of the environment, _contextual bandits_ output an action based on the context.\n",
    "\n",
    "__Mushroom Case Stuty:__ <br>\n",
    "We are provided with a list of 8124 mushrooms, each having 22 features (characteristcs of the mushroom) and 1 label (poisonous or edible). Our agent can carry out 2 actions: eat a mushroom or not eat a mushroom. The problem context is the vector of features which is associated with the mushroom which the agent is about to eat/non eat.\n",
    "If our agent eats an edible mushroom, it receives a reward of 5. If the agent eats a poisonous mushroom, it receives a reward of -35 with 0.5 probability and a reward of 5 with 0.5 probaility. If the agent doesn't eat, it receives a reward o 0.\n",
    "\n",
    "We are also provided an _oracle_. The oracle always selects the right action, receiving a reward of 5 when it eats an edible mushroom, and a reward of 0 when it doesn't eat.\n",
    "\n",
    "__Objective:__ <br>\n",
    "Create a BNN which minimises the _cumulative regret_ of the agent. Regreat measures the difference between the oracle and our agent's reward.\n",
    "\n",
    "__BNN Architecture:__\n",
    "- 2 hidden layers\n",
    "- Each layer has 100 rectified units\n",
    "- Vector input: vector consisting of the mushroom features (context) and a one of _K_ encoding of the action\n",
    "- Output: single scalar representing the expected reward of the given action in the given context\n",
    "\n",
    "__Additional Important Information:__\n",
    "- To calculate the expected reward for an action, Google samples twice the weights and averages the outputs\n",
    "- To train the network, Google keeps a buffer of 4096 mushrooms.\n",
    "- For training, Google randomly draws minibatches of size 64 for 64 training steps.\n",
    "- After every training stage, the agent interacts with a new mushroom\n",
    "\n",
    "## TASKS:\n",
    "- Data preparation: DONE\n",
    "- Reward function: DONE\n",
    "- Initialise buffer function: DONE\n",
    "- Update buffer function: DONE\n",
    "- Create class for Gaussian distribution: DONE\n",
    "- Create class for generating prior distribution: DONE\n",
    "- Create class for generating BNN layer: DONE\n",
    "- Create BNN: Need to change\n",
    "- Create function for creating network estimates: TO BE DONE\n",
    "- Create function for updating network parameters: TO BE DONE\n",
    "\n",
    "\n",
    "### Algorithm for network estimtes:\n",
    "##### Inputs: network and mushroom - Returns rewards for both actions\n",
    "def network-estimates(network, mushroom): <br>\n",
    "    sample w1,w2<br>\n",
    "    r1 = 0.5 * (f(mushroom,eat, w1) + f(mushroom,don't eat, w1))<br>\n",
    "    r2 = 0.5 * (f(mushroom,eat, w2) + f(mushroom,don't eat, w2))<br>\n",
    "    return r1,r2<br>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available?:  False\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from BayesBackpropagation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Preparation:__ ## \n",
    "Input is the data file. Output is an array of one-hot encoded labels (y, shape: 8124) and an array of context features (x, shape: 8124 elements, each with 117 features). 117 features derive from doing a one-hot encoding on the feature classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:380: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. The passed value of 'all' is the default and can simply be removed.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import data from file\n",
    "df = pd.read_csv(os.getcwd() + '/agaricus-lepiota.data', sep=',', header=None,\n",
    "             error_bad_lines=False, warn_bad_lines=True, low_memory=False)\n",
    "\n",
    "# Set pandas to output all of the columns in output\n",
    "df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n",
    "         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n",
    "\n",
    "# Split context from label\n",
    "X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n",
    "# Put the class values (0th column) into Y\n",
    "Y = df['class']\n",
    "\n",
    "# Transform labels into one-hot encoded array\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "y = le.transform(Y)\n",
    "\n",
    "# Temporary variable to avoid error \n",
    "x_tmp = pd.DataFrame(X,columns=[X.columns[0]])\n",
    "\n",
    "# Encode each feature column and add it to x_train \n",
    "for colname in X.columns:\n",
    "    le.fit(X[colname])\n",
    "    #print(colname, le.classes_)\n",
    "    x_tmp[colname] = le.transform(X[colname])\n",
    "\n",
    "# Produce mushroom array: 8124 mushrooms, each with 117 one-hot encoded features\n",
    "oh = preprocessing.OneHotEncoder(categorical_features='all')\n",
    "oh.fit(x_tmp)\n",
    "x = oh.transform(x_tmp).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Reward function:__ ##\n",
    "Implemented as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 117)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWARD FUNCTION\n",
    "\n",
    "def get_reward(eaten, edible):\n",
    "    # REWARDS FOR AGENT\n",
    "    #  Eat poisonous mushroom\n",
    "    if not eaten:\n",
    "        return 0\n",
    "    if eaten and edible:\n",
    "        return 5\n",
    "    elif eaten and not edible:\n",
    "        return (5 if np.random.rand() > 0.5 else -35)\n",
    "\n",
    "def oracle_reward(edible):\n",
    "    return 5*edible    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mushroom Buffer\n",
    "Generate buffer to keep 4096 elements used to train the Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_buffer():\n",
    "    bufferX, bufferY = [], []\n",
    "    for i in np.random.choice(range(len(x)), 4096):\n",
    "        eat = np.random.rand()>0.5\n",
    "        bufferX.append(np.concatenate((x[i], [1, 0] if eat else [0, 1])))\n",
    "        bufferY.append(get_reward(eat, y[i]))\n",
    "    return bufferX, bufferY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available?:  False\n"
     ]
    }
   ],
   "source": [
    "# Define some hyperparameters\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "print(\"Cuda available?: \",torch.cuda.is_available())\n",
    "\n",
    "PI = 0.5\n",
    "SIGMA_1 = torch.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.FloatTensor([math.exp(-6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "Var = lambda x, dtype=torch.FloatTensor: Variable(torch.from_numpy(x).type(dtype))\n",
    "\n",
    "\n",
    "class MushroomNet():\n",
    "    def __init__(self, label='MushNet'):\n",
    "        self.label = label\n",
    "        self.epsilon = 0\n",
    "        self.net = None\n",
    "        self.bufferX, self.bufferY = init_buffer()\n",
    "        self.loss, self.optimizer = None, None\n",
    "        self.cum_regrets = [0]\n",
    "    \n",
    "    def expected_rewards(self, context, k=2):\n",
    "        c_eat = Var(np.concatenate((context, [1, 0])))\n",
    "        c_reject = Var(np.concatenate((context, [0, 1])))\n",
    "        with torch.no_grad():\n",
    "            r_eat = np.mean([self.net.forward(c_eat).numpy().reshape(1)[0] for _ in range(k)])\n",
    "            r_reject = np.mean([self.net.forward(c_reject).numpy().reshape(1)[0] for _ in range(k)])\n",
    "        return r_reject, r_eat\n",
    "\n",
    "    def try_(self, mushroom):\n",
    "        context, edible = x[mushroom], y[mushroom]\n",
    "        r_reject, r_eat = self.expected_rewards(context, k=1)\n",
    "        eaten = r_eat > r_reject\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            eaten = (np.random.rand()<.5)\n",
    "        reward = get_reward(eaten, edible)\n",
    "        action = [1, 0] if eaten else [0, 1]\n",
    "        self.bufferX.append(np.concatenate((context, action)))\n",
    "        self.bufferY.append(reward)\n",
    "        rg = oracle_reward(edible) - reward\n",
    "        self.cum_regrets.append(self.cum_regrets[-1]+rg)\n",
    "    \n",
    "    def update(self, mushroom):\n",
    "        self.try_(mushroom)\n",
    "        bX = Var(np.array(self.bufferX[-4096:]))\n",
    "        bY = Var(np.array(self.bufferY[-4096:]))\n",
    "        for idx in np.split(np.random.permutation(range(4096)), 64):\n",
    "            self.net.train()\n",
    "            self.net.zero_grad()\n",
    "            self.loss(bX[idx], bY[idx]).backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBB_MNet(MushroomNet):\n",
    "    def __init__(self, label):\n",
    "        super().__init__(label)\n",
    "        self.net = BayesianNetwork(inputSize = x.shape[1]+2,\n",
    "                          CLASSES = 1, \n",
    "                          layers=np.array([100,100]), \n",
    "                          activations = np.array(['relu','relu','none']), \n",
    "                          SAMPLES = 1, \n",
    "                          BATCH_SIZE = 64,\n",
    "                          NUM_BATCHES = 64).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.net.parameters())\n",
    "        self.loss = lambda data, target:self.net.sample_elbo(data, target)[0]\n",
    "        \n",
    "        \n",
    "\n",
    "class EpsGreedyMlp(MushroomNet):\n",
    "    def __init__(self, label, epsilon=0):\n",
    "        super().__init__(label)\n",
    "        self.epsilon = epsilon\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Linear(x.shape[1]+2, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 1))\n",
    "        self.bufferX, self.bufferY = init_buffer()\n",
    "        self.optimizer = optim.Adam(self.net.parameters())\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.loss = lambda data, target: self.mse(self.net.forward(data), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "fig.suptitle('Cumulative Regrets')\n",
    "\n",
    "\n",
    "mushroom_nets = {'bbb':BBB_MNet(label = 'Bayes By BackProp'),\n",
    "                 'e0':EpsGreedyMlp(epsilon=0, label = 'Greedy'),\n",
    "                 'e1':EpsGreedyMlp(epsilon=0.01, label = '1 Greedy'),\n",
    "                 'e5':EpsGreedyMlp(epsilon=0.05, label = '5 Greedy')}\n",
    "NB_STEPS = 200\n",
    "for _ in range(NB_STEPS):\n",
    "    ax.clear()\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Cumulative Regret')\n",
    "    mushroom = np.random.randint(len(x))\n",
    "    for key, net in mushroom_nets.items():\n",
    "        net.update(mushroom)\n",
    "        ax.plot(net.cum_regrets, label = net.label)\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
